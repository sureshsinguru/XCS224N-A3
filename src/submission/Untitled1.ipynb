{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9b9be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4ffdf60fac13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mparser_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from .parser_utils import load_and_preprocess_data\n",
    "\n",
    "\n",
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "                 hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### TODO:\n",
    "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###\n",
    "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        ###         It has been shown empirically, that this provides better initial weights\n",
    "        ###         for training networks than random uniform initialization.\n",
    "        ###         For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
    "        ### Hints:\n",
    "        ###     - After you create a linear layer you can access the weight\n",
    "        ###       matrix via:\n",
    "        ###         linear_layer.weight\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        ### START CODE HERE (~5 Lines)\n",
    "        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, hidden_size)\n",
    "        self.dropout = nn.Dropout(p = self.dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(hidden_size,self.n_classes)\n",
    "        nn.init.xavier_uniform_(self.embed_to_hidden.weight,gain=1)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logits.weight,gain=1)\n",
    "        ### END CODE HERE\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
    "            to embedding vectors.\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
    "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
    "\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        ### TODO:\n",
    "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
    "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
    "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        ###  Please see the following docs for support:\n",
    "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        ### START CODE HERE (~1-3 Lines)\n",
    "        tmp_features = self.pretrained_embeddings(t)\n",
    "        shape = tmp_features.size()\n",
    "        x = tmp_features.view(shape[0],shape[1]*shape[2])\n",
    "        ### END CODE HERE\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
    "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                - For more details checkout: `https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### TODO:\n",
    "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        ###     4) Apply dropout layer to the output of step 3.\n",
    "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu\n",
    "        ###  START CODE HERE (~3-5 lines)\n",
    "        x = self.embedding_lookup(t)\n",
    "        x = self.embed_to_hidden(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.hidden_to_logits(x)\n",
    "        ### END CODE HERE\n",
    "        return logits\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    _, embeddings, _, _, _ = load_and_preprocess_data()\n",
    "    model = ParserModel(embeddings)\n",
    "    t = test_case.test_cases_ip['parser_model']['t']\n",
    "    model_actual = model.forward(t).data.numpy().tolist()\n",
    "    model_expected = test_case.test_cases_op['parser_model']\n",
    "    print(\"actual output\")\n",
    "    pprint(model_actual)\n",
    "    print(\"\\n\" * 2)\n",
    "    print(\"expected output\")\n",
    "    pprint(model_expected)\n",
    "\n",
    "    print(\"test passed\" if np.isclose(model_actual, model_expected, atol=1e-2).all() else \"test failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff8a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a75353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
